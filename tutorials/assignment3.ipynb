{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "81e1b977-fa86-44d7-b5f0-a6f1cca78ab3"
    }
   },
   "source": [
    "<img style=\"float: right;\" src=\"http://www2.le.ac.uk/liscb1.jpg\">  \n",
    "# Leicester Institute of Structural and Chemical Biology: Python for Biochemists\n",
    "# Assignment 3: Curve fitting\n",
    "\n",
    "Scipy is the first place to look for general-purpose scientific functionality.  The Scipy library is enormous and varied, so covering all the features is a course unto itself.  Having said that, one of the most commonly used features in scipy is it's fitting routines, which we will now explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92136e88-e29e-4111-8750-c88ebb58bb51"
    }
   },
   "source": [
    "## Simple Curve fitting\n",
    "\n",
    "Scipy comes with a number of fitting routines.  One that can be extremely useful is `curve_fit`, which fits a function to a given set of data using a least-squares minimization.  Here, we'll fit some (made up) enzyme kinetics data to the Michaelis-Menten equation:  \n",
    "\n",
    "$$\\large V = \\frac{V_{max}[S]}{K_m+[S]}$$  \n",
    "\n",
    "Where $V$ is the initial rate of the reaction, $V_{max}$ is the rate of the reaction with infinite substrate concentration, $[S]$ is the substrate concentration and ${K_m}$ is the so-called *Michaelis* constant.\n",
    "\n",
    "The data fitted are a set of values of $V$ measured at various values of $[S]$.\n",
    "\n",
    "The parameters to be found by the fitting are $V_{max}$ and ${K_m}$.\n",
    "\n",
    "First, we define the Michaelis-Menten function, which returns the values of $V$ for a series of values of $[S]$. The function accepts in input the values of $[S]$ (abscissae) and the two parameters of the Michaelis-Menten equation, $V_{max}$ and ${K_m}$.\n",
    "\n",
    "Next - we shall simulate some data - using a pair of $V_{max}$ and ${K_m}$ fixed values.\n",
    "\n",
    "Then - we shall add noise to our data.\n",
    "\n",
    "Finally - we shall use `curve_fit` least-squares fitting against our noisy data to obtain values of $V_{max}$ and ${K_m}$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e7a24fae-02ea-4f7e-855f-96196cf04ad1"
    }
   },
   "outputs": [],
   "source": [
    "def michaelis_menten(s, km, vmax):\n",
    "    return (vmax*s) / (km + s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "384fdd93-16fc-4462-af13-f4161ead77bb"
    }
   },
   "source": [
    "Now - in order to simulate enzymatic measurements of $V$ - we pick some substrate concentrations $[S]$ we (would) do our measurements at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "adb5385b-c9ed-41c0-945d-4041ac358d52"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "059ccecf-4c19-4e9b-89c0-de7b3d9310ad"
    }
   },
   "outputs": [],
   "source": [
    "substrate_concentrations = np.array([0.01, 0.05, 0.1, 0.5, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "af264ed4-3682-48f5-baf0-6a75b865eca9"
    }
   },
   "source": [
    "As well as 6 data points, we also want to evaluate the initial velocity for a larger number of closely spaced values of $[S]$.\n",
    "\n",
    "For this purpose, we'll create a numpy array of 1000 values between x=0 M and x=3 M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "277a693f-c70b-42e0-b9bf-104c78772129"
    }
   },
   "outputs": [],
   "source": [
    "substrate_concentration_range = np.linspace(0, 3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9794ff9d-74b9-43f9-ba34-96ed4ca1486d"
    }
   },
   "source": [
    "Ok, lets make sure this looks right:\n",
    "\n",
    "0. For our simulated data - we shall choose arbitrarily ${K_m}$=0.1 M and $V_{max}$ =0.5 M/s\n",
    "1. We'll use plt.scatter to plot the six values of $V$ calculated for the six values of $[S]$. These 6 calculated $V$ values will be stored in an array called initial_velocities.\n",
    "2. We'll use plt.plot  to plot $V$ calculated for the 1000 values of $[S]$ between 0 and 3 M. These 1000 calculated values of $V$ will be stored in an array called mm_curve. \n",
    "\n",
    "As well as the values of $V$ - we shall also plot:\n",
    "\n",
    "3. A vertical dashed line reaching the MM curve from $V$=0 at abscissa $[S]$=${K_m}$\n",
    "4. A horizontal dashed line stretching from $[S]$=0 to the MM curve at ordinate $V$=0.5*$V_{max}$ \n",
    "\n",
    "For the latter two plots we shall use the Pyplot.vlines and Pyplot.hlines methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c70edfd7-6502-4b2d-a01f-909c07343d5a"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "15fb9b87-fd8b-49be-8434-ceca9504b7d8"
    }
   },
   "outputs": [],
   "source": [
    "km = 0.1\n",
    "vmax = 0.5\n",
    "\n",
    "initial_velocities = michaelis_menten(s=substrate_concentrations, km=km, vmax=vmax)\n",
    "mm_curve = michaelis_menten(substrate_concentration_range, km, vmax)\n",
    "\n",
    "\n",
    "# Plot all the things!\n",
    "plt.scatter(substrate_concentrations, initial_velocities)\n",
    "plt.plot(substrate_concentration_range, mm_curve)\n",
    "\n",
    "plt.title(\"Calculated noiseless data\")\n",
    "plt.ylabel('Initial velocity V (M/s)')\n",
    "plt.xlabel('Substrate concentration [S] (M)')\n",
    "\n",
    "# In M-M kinetics, the Km is the substrate concentration where you've reached half-max rate\n",
    "plt.vlines(km, ymin=0, ymax=vmax/2, linestyle='dashed')\n",
    "plt.hlines(vmax/2, xmin=0, xmax=km, linestyle='dashed')\n",
    "\n",
    "plt.xlim(xmin=0, xmax=3)\n",
    "plt.ylim(ymin=0, ymax=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "91659714-0323-4e6b-a690-b8bdb8fa66f3"
    }
   },
   "source": [
    "Ok, so lets now simulate some noisy data (with Normally distributed noise).\n",
    "\n",
    "The np.random.normal(loc=l, scale=s, size=n) method will draw n random samples from a normal (Gaussian) distribution centred around loc=l and standard deviation scale=s \n",
    "(see https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html)\n",
    "\n",
    "We choose loc=0 and scale=0.1 and we add noise to each piece of simulated data in proportion to its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fe914a33-f391-4466-a63b-c1a75a242002"
    }
   },
   "outputs": [],
   "source": [
    "number_of_concentrations = len(substrate_concentrations)\n",
    "noise = (np.random.normal(loc=0, scale=0.1, size=number_of_concentrations))\n",
    "simulated_data = michaelis_menten(substrate_concentrations, km, vmax)\n",
    "simulated_noisy_data = simulated_data + (noise*simulated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8b149f2f-ffd7-4b91-8510-cc5227c01078"
    }
   },
   "source": [
    "ok, so how does our simulation look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bd13d505-6702-44ea-9be3-619bbccffa2a"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(substrate_concentration_range, mm_curve, label='Noiseless data')\n",
    "plt.scatter(substrate_concentrations, simulated_noisy_data, label='Noisy data', color='green')\n",
    "plt.legend(loc='lower right');\n",
    "\n",
    "plt.title(\"Noiseless and noisy data\")\n",
    "plt.ylabel('Initial velocity V (M/s)')\n",
    "plt.xlabel('Substrate concentration [S] (M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bd5046a4-4a31-4c56-a058-53d7dcb50be2"
    }
   },
   "source": [
    "Now we can try to fit a MM curve to the noisy data we have generated.  In order to do this, we need to provide a guess for the initial values of $V_{max}$ and $K_m$ parameters - so that the algorithm can start from somewhere.  \n",
    "\n",
    "For something as simple as the M-M equation, even quite bad guesses will do. We'll set the starting value of $V_{max}$ to 100 M/s and the starting value for $K_m$ to the last of our simulated_noisy_data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "18bc67a5-1745-42e7-963b-ade414db379b"
    }
   },
   "outputs": [],
   "source": [
    "initial_guess = (100, simulated_noisy_data[-1])  # Km and Vmax.  Note the Km is a truly horrible guess, given our data\n",
    "print(initial_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c571e13e-4227-4ffc-84f9-1c95b8eef6a9"
    }
   },
   "source": [
    "In addition, we can provide the fitting algorithm with bounds - regions of allowed values for the parameters.  In this case, it's not necessary, but it's always good to have a sanity check (in this case, both parameters must be positive and we assume no upper bound.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d9ccad30-f3be-47bb-accf-91704a1987d9"
    }
   },
   "outputs": [],
   "source": [
    "lower_bounds = (0, 0)  # Km, Vmax\n",
    "upper_bounds = (np.inf, np.inf)  # Km, Vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c05ec43a-8a08-4daa-a925-5a5cfac6327f"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "fitted, covariance = curve_fit(f=michaelis_menten,\n",
    "                               xdata=substrate_concentrations,\n",
    "                               ydata=simulated_noisy_data,\n",
    "                               p0=initial_guess,\n",
    "                               bounds=(lower_bounds, upper_bounds)\n",
    "                              )\n",
    "print('Km:', fitted[0])\n",
    "print('Vmax:',fitted[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c5f2063a-188a-434c-a8de-9cdd40798f33"
    }
   },
   "source": [
    "Remember, always look at your data as much as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "75495aa9-aca6-46ec-bbd0-1621dbd2494b"
    }
   },
   "outputs": [],
   "source": [
    "calculated_curve = michaelis_menten(substrate_concentration_range, fitted[0], fitted[1])\n",
    "\n",
    "plt.plot(substrate_concentration_range, mm_curve, label='Noiseless Data')\n",
    "plt.scatter(substrate_concentrations, simulated_noisy_data, label='Noisy Data', color='green')\n",
    "plt.plot(substrate_concentration_range, calculated_curve, label='Fit to Noisy Data')\n",
    "\n",
    "plt.legend(loc='lower right');\n",
    "\n",
    "plt.title(\"Noiseless and fit to noisy data\")\n",
    "plt.ylabel('Initial velocity V (M/s)')\n",
    "plt.xlabel('Substrate concentration [S] (M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Did the fitting actually work?\n",
    "The easiest way to see if the fitting function returned sensible values is not to just look at the curves!  Instead, the best way is to randomize the starting point. If it keeps giving the same numbers from various starting positions, it's more likely you can trust the results.\n",
    "1. Generate a random starting value for $K_m$ from 0.01 to 10, and for $V_{max}$ from 0.01 to 10.  *Hint: use `random.uniform()`\n",
    "2. Fit using these starting values and print both the starting value and resulting parameters.\n",
    "3. Repeat for 10 total fits.  How does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9b7729a3-b3ea-4d49-9c46-a34c87695884"
    }
   },
   "source": [
    "### Exercise 2:  Compute the fitting error\n",
    "Compute the errors on the fitted values of $V_{max}$ and $K_m$. \n",
    "\n",
    "Curve_fit computes the covariance matrix of the fitted parameters with the method pcov - the pcov matrix is returned by Curve_fit together with the fitted parameters.\n",
    "\n",
    "The square root of each diagonal element of the covariance matrix can be computed to obtain the error estimated on the corresponding fitted parameter:\n",
    "\n",
    "perr = np.sqrt(np.diag(covariance))\n",
    "\n",
    "See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n",
    "\n",
    "or read the docstring of curve_fit using `.?`, and look at what the function returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remake the plot, but shade the error bounds: \n",
    "1. The MM curve corresponding to the maximum error in the fitted parameters can be computed with $K_m$'=$K_m$-err($K_m$) and $V_{max}$'=$V_{max}$+err($V_{max}$)\n",
    "2. The MM curve corresponding to the minimimum error in the fitted parameters can be computed with $K_m$\"=$K_m$+err($K_m$) and $V_{max}$\"=$V_{max}$-err($V_{max}$)\n",
    "3. Calculate the curve for the maximum and minimum errors as we did above\n",
    "4. Ask google how to fill between curves in matplotlib: a common application for plt.fill_between is the indication of confidence bands. fill_between uses the colors of the color cycle as the fill color. These may be a bit strong when applied to fill areas. It is therefore often a good practice to lighten the color by making the area semi-transparent using alpha. Try setting `alpha=0.2` in the plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b4cde2a2-cea2-4c0e-9017-93436d9bb3bd"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2055d2e7-cc65-47c6-9b6d-e6cca15ffc8a"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "bc357582-a3fc-49e2-9024-8cf4f2e94f65"
    }
   },
   "source": [
    "## Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "63f91388-d1aa-4ebd-921b-b64ffc6e02ef"
    }
   },
   "source": [
    "In the fit above, we used the covariance matrix to determine the error bounds.  What's not so apparent from this is that this only works for some fitting algorithm (including linear least-squares - by-far the most commonly used fitting algorithm), is only valid if the errors are normally distributed, and is then only valid if the data is homoscedastic (the measurement error is the same for every point - which isn't really true in this case!)  \n",
    "\n",
    "So what are we to do?\n",
    "1.  Work out the analytical error for every fit, assuming the characteristics of the error (hard - lots of algebra)\n",
    "2.  Use resampling to empirically determine the distribution of parameter values (hard - lots of computation)\n",
    "\n",
    "Except #2 isn't hard - we have computers to do the computation for us.  The type of resampling we'll be doing is call bootstrapping.  The mathematical justification for why this works is (way) beyond this course.  But the important result is that, if you follow a few rules, you can approximate the actual underlying distribution on almost any parameter, and then extract what you want directly.  Here are the rules:\n",
    "1. Generate a new set of data by sampling, with replacement, from your current data set.\n",
    "2. The new data set must have the same number of observations as the original (some data will get repeated, some left out)\n",
    "3. Fit your curve with the new dataset\n",
    "4. Repeat a few hundred to tens of thousands of times until the distribution stabilizes\n",
    "\n",
    "An example in code should make this clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bootstrap_sample(original_sample_x, original_sample_y):\n",
    "    assert len(original_sample_x) == len(original_sample_y)\n",
    "    indices = np.arange(len(original_sample_x)-1)\n",
    "    resample_indices = np.random.choice(indices, size=len(indices), replace=True)\n",
    "    resampled_x = original_sample_x[resample_indices]\n",
    "    resampled_y = original_sample_y[resample_indices]\n",
    "    return resampled_x, resampled_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bounds = (0, 0)  # Km, Vmax\n",
    "upper_bounds = (np.inf, np.inf)  # Km, Vmax\n",
    "initial_guess = (1, simulated_noisy_data[-1])  # Km and Vmax.\n",
    "\n",
    "bootstrap_values =[]\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Generate new sample indices with replacement \n",
    "    resampled_s, resampled_d = make_bootstrap_sample(substrate_concentrations, simulated_noisy_data)\n",
    "    \n",
    "    # Fit the curve with the resampled dataset\n",
    "    bootstrap_fitted, bootstrap_covariance = curve_fit(f=michaelis_menten,\n",
    "                                                       xdata=resampled_s,\n",
    "                                                       ydata=resampled_d,\n",
    "                                                       p0=initial_guess,\n",
    "                                                       bounds=(lower_bounds, upper_bounds)\n",
    "                                                      )\n",
    "    bootstrap_values.append(bootstrap_fitted)\n",
    "    \n",
    "bootstrap_kms = tuple(v[0] for v in bootstrap_values)\n",
    "bootstrap_vmaxs = tuple(v[1] for v in bootstrap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(bootstrap_kms, bins=100)\n",
    "# plt.xlim(0,1)\n",
    "plt.title(f'Km: {len(bootstrap_kms)} resamples')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(bootstrap_vmaxs, bins=100)\n",
    "# plt.xlim(0,2)\n",
    "plt.title(f'Vmax: {len(bootstrap_vmaxs)} resamples');\n",
    "\n",
    "len(bootstrap_kms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the plots above aren't that smooth, and have some outliers.  Lets repeat the bootstrap with 10 000 iterations (start this running now, while you read the instructions for Exercise 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bounds = (0, 0)  # Km, Vmax\n",
    "upper_bounds = (np.inf, np.inf)  # Km, Vmax\n",
    "initial_guess = (1, simulated_noisy_data[-1])  # Km and Vmax.\n",
    "\n",
    "bootstrap_values =[]\n",
    "\n",
    "iterations = 10000\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Generate new sample indices with replacement \n",
    "    resampled_s, resampled_d = make_bootstrap_sample(substrate_concentrations, simulated_noisy_data)\n",
    "    \n",
    "    # Fit the curve with the resampled dataset\n",
    "    bootstrap_fitted, bootstrap_covariance = curve_fit(f=michaelis_menten,\n",
    "                                                       xdata=resampled_s,\n",
    "                                                       ydata=resampled_d,\n",
    "                                                       p0=initial_guess,\n",
    "                                                       bounds=(lower_bounds, upper_bounds)\n",
    "                                                      )\n",
    "    bootstrap_values.append(bootstrap_fitted)\n",
    "    \n",
    "bootstrap_kms = tuple(v[0] for v in bootstrap_values)\n",
    "bootstrap_vmaxs = tuple(v[1] for v in bootstrap_values)\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(bootstrap_kms, bins=100)\n",
    "# plt.xlim(0,1)\n",
    "plt.title(f'Km: {len(bootstrap_kms)} resamples')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(bootstrap_vmaxs, bins=100)\n",
    "# plt.xlim(0,2)\n",
    "plt.title(f'Vmax: {len(bootstrap_vmaxs)} resamples');\n",
    "\n",
    "len(bootstrap_kms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Confidence interval\n",
    "The 90% confidence interval is the range of values that contain the central 90% of the distribution.  Extract the 90%  CI for the data above.\n",
    "1. Make sure you've started the 10 000 iterations above.\n",
    "2. Make sorted listes of bootstrap_kms and bootstrap_vmaxs\n",
    "3. If you have 10000 sorted data points, the central 9000 data points make up your 90% CI - extract the lowest and hightest values from these 9000\n",
    "4. Use `plt.scatter` to plot all 10000 points.  What do you notice?\n",
    "4. Eliminate any datapoint with $K_m$ or $Vmax$ values outside the 90% central data, and polt these points.  What do you notice?\n",
    "5. Make two plots comparing the fitting error to the 90% CI.  First, plot the $K_m$ CI using the originally fitted $V_{max}$, then plot the $V_{max}$ CI using the originally fitted $K_m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note here:  The confidence intervals *are not centered on the prediction*, this means that, given our data, we're more certain about one bound than the other.  Because we've used random noise, your intervals may appear centered, but if you run the notebook again, you may be able to see that they're not.\n",
    "\n",
    "So what's going on here?  Why did our \"Fitting Error\" above give us a single bound centered on the data?  The covariance matrix method assumes the data is normally distributed - so the error with be the same both 'above' and 'below' the true value.  It also assumes that the X and Y values are NOT correlated - which they are here.  \n",
    "\n",
    "So which to use?  If you want to know how good the program fit your data, use the covariance method.  If you want to know how well your experiment's determined the true parameters, use the bootstrap.  It really is that simple.\n",
    "\n",
    "Finally - what can we do to improve our precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Experimental replicates\n",
    "Most experiments will have replicate data points, i.e. the researcher measures the initial rate several times at each substrate concentration.\n",
    "1. Create a new list of substrate concentrations called `substrate_concentrations_with_replicates`, using each concentration three times (order doesn't matter)\n",
    "2. Create new simulated data called `simulated_data_with_replicates` from above, adding the same *amount* of random noise (but not the same set of random numbers) as for `simulated_noisy_data`\n",
    "3. Fit the data and calculate the 90% CI with the replicates.  Make plots of the distributions.\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d3b60900-9f96-4f2b-b7d1-962102b13410"
    }
   },
   "source": [
    "## Model choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e64b70b6-ff93-4a2b-88fa-12da31ddc7c1"
    }
   },
   "source": [
    "One common difficulty in analysing data is choosing the correct model.  The most tempting choice is the model that fits our data best - *this is extremely dangerous.*  Here's the equation for homotropic alosterism (substrate activation or inhibition of the enzyme):  \n",
    "\n",
    "$$\\large V = \\frac{V_{max}\\frac{[S]}{K_{D1}} + \\beta V_{max}\\frac{[S]^2}{\\alpha K_{D1}K_{D12}}}{1 + \\frac{[S]}{K_{D1}} + \\frac{[S]}{K_{D2}} + \\frac{[S]^2}{\\alpha K_{D1} K_{D2}}}$$  \n",
    "\n",
    "Now, **we know that our system doesn't have any substrate inhibition or activation** - we created the data afterall - but lets see how this does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c3a995f5-78ba-49c3-b74c-ac4e15c513d8"
    }
   },
   "outputs": [],
   "source": [
    "def michaelis_menten_ha(s, kd1, kd2, alpha, beta, vmax):\n",
    "    '''\n",
    "    The Michaelis-Menton function including homotropic allosterism. \n",
    "    '''\n",
    "    numerator = vmax * (s/kd1) + (beta * vmax * (s**2)/(alpha * kd1 *kd2))\n",
    "    denominator = 1 + (s/kd1) + (s/kd2) + (s**2)/(alpha * kd1 * kd2)\n",
    "    \n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eac752f9-c35b-473f-b3fb-c192f0ea22c6"
    }
   },
   "source": [
    "If we set $\\alpha$ and $\\beta$ to be 1, and set $K_{D1}$ and $K_{D2}$ to the same value, the equation above reduces to the standard Michaelis-Menton equation, so we'll set our starting parameters there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "323208b9-f96b-4fb9-9214-7791ac5ecbab"
    }
   },
   "outputs": [],
   "source": [
    "initial_guess = (fitted[0], fitted[0], 1, 1, fitted[1])  # KD1, KD2, alpha, beta and Vmax\n",
    "lower_bounds_ha = (0, 0, 0, 0, 0)\n",
    "upper_bounds_ha = (np.inf, np.inf, np.inf, np.inf, np.inf)\n",
    "\n",
    "fitted_ha, covariance_ha = curve_fit(f=michaelis_menten_ha,\n",
    "                               xdata=substrate_concentrations,\n",
    "                               ydata=simulated_noisy_data,\n",
    "                               p0=initial_guess,\n",
    "                               bounds=(lower_bounds_ha, upper_bounds_ha)\n",
    "                              )\n",
    "\n",
    "print('Original Michaelis-Menton')\n",
    "print('Km:', fitted[0])\n",
    "print('Vmax:',fitted[1])\n",
    "\n",
    "print()\n",
    "\n",
    "print('Michaelis-Menton with aditional parameters')\n",
    "print('KD1', fitted_ha[0])\n",
    "print('KD2:',fitted_ha[1])\n",
    "print('alpha:', fitted_ha[2])\n",
    "print('beta:',fitted_ha[3])\n",
    "print('Vmax:', fitted_ha[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e049ca4f-824b-4819-89c6-9db72af79472"
    }
   },
   "source": [
    "Ok, so now which one fits our data better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fbb7a234-1852-4b76-8e66-5069135f765e"
    }
   },
   "outputs": [],
   "source": [
    "calculated_curve_ha = michaelis_menten_ha(substrate_concentration_range,\n",
    "                                          *fitted_ha)  # *fitted_ha => fitted_ha[0], fitted_ha[1]\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(substrate_concentrations, simulated_noisy_data, label='simulated data', color='green')\n",
    "plt.plot(substrate_concentration_range, calculated_curve, label='Fitted Michaelis-Menton', color='red')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(substrate_concentrations, simulated_noisy_data, label='simulated data', color='green')\n",
    "plt.plot(substrate_concentration_range, calculated_curve_ha, label='Fitted Michaelis-Menton-HA', color='purple')\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1988469e-94f4-4fc8-9246-da6e4cd12ecb"
    }
   },
   "source": [
    "Which one do you think looks better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does this mean?  Because the  `michaelis_menten_ha` function is the `michaelis_menten` function with three extra parameters, it will **always** fit noisy data better.  Because we used the `michaelis_menten` function to generate the data, we know that's the one we should actually be using, though.  If you're faced with a real-world problem, and the function with more parameters will always fit noisy data better, how can you tell if you really should use the function with more parameters or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dba9a945-fc3c-4f08-84bb-d90e073d3a4f"
    }
   },
   "source": [
    "## Model selection by cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you know which model to choose?  One of the best ways is cross-validation.  The key to undersanding why this is so powerful is that cross-validation essentially tests how much of the random noise you're modelling.  The function that models the least noise and most signal is almost always the correct one. \n",
    "\n",
    "Cross-validation leaves a bit of data out, then fits (trains) the curve without that data, then tests how well the left out data is predicted by the fit.\n",
    "\n",
    "We'll start with leave-one-out cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = (100, simulated_noisy_data[-1])  # Km and Vmax.  Note the Km is a truly horrible guess, given our data\n",
    "lower_bounds = (0, 0)  # Km, Vmax\n",
    "upper_bounds = (np.inf, np.inf)  # Km, Vmax\n",
    "predictions = []\n",
    "\n",
    "# Step through each substrate concentration\n",
    "for i in range(len(substrate_concentrations)):\n",
    "    \n",
    "    # Build a mask excluding the test measurement\n",
    "    mask = np.ones(len(substrate_concentrations)).astype('bool')\n",
    "    mask[i] = False\n",
    "    \n",
    "    # Remove the test measurement from our training data\n",
    "    training_substrate_concentrations = substrate_concentrations[mask]\n",
    "    training_simulated_data = simulated_noisy_data[mask]\n",
    "    \n",
    "    # Fit the curve\n",
    "    bootstrap_fitted, bootstrap_covariance = curve_fit(f=michaelis_menten,\n",
    "                                                       xdata=training_substrate_concentrations,\n",
    "                                                       ydata=training_simulated_data,\n",
    "                                                       p0=initial_guess,\n",
    "                                                       bounds=(lower_bounds, upper_bounds)\n",
    "                                                      )\n",
    "    predictions.append(michaelis_menten(substrate_concentrations[i], *fitted))\n",
    "\n",
    "abs_errors = abs(predictions - simulated_noisy_data)\n",
    "xval_error = sum(abs_errors)\n",
    "print(\"Cross validation error for 'michaelis_menten':\",xval_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Model selection by cross validation\n",
    "Run the cross validation on `michaelis_menten_ha`.  Which function gives lower cross-validation error?  \n",
    "\n",
    "*Hint: If you get `Optimal parameters not found` error, add the parameter `max_nfev=10000` to the `curve_fit` call.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So which model do we choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval_ratio = xval_error / xval_ha_error\n",
    "\n",
    "if xval_ratio < 0.95:\n",
    "    print('Michaelis-Menton is the prefered model.')\n",
    "elif 1/xval_ratio < 0.95:\n",
    "    print('Michaelis-Menton with Homotropic Alostery is the prefered model.')\n",
    "else:\n",
    "    print(\"The cross validation scores are too close, we can't descriminiate between the models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, there are lots of choices for how exactly we do cross-validation, but here are the two most import issues:\n",
    "\n",
    "1. If you only have a few more observations than parameters, do leave-one-out validation.\n",
    "2. If you have correlated data (i.e. several blocks of data,) try to keep the correlated data either in the test or training sets  \n",
    "\n",
    "In general, if you have many more data points than parameters, you can do k-fold cross validation.  For example, with 5-fold cross-validation, you would split your data into 5 chunks, using 4 of the chunks for training and one for error determination.  As you have 5 chunks, you do it 5 times (using a different chunk each time,) and sum up the errors.  The model with the lowest error is the one over-fitting the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
